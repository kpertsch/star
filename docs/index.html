

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- ======================================================================= -->
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<style type="text/css">
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 100%;
  }

  h1 {
    font-weight:300;
  }

  div {
    max-width: 95%;
    margin:auto;
    padding: 10px;
  }

  .table-like {
    display: flex;
    flex-wrap: wrap;
    flex-flow: row wrap;
    justify-content: center;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img {
    padding: 0;
    display: block;
    margin: 0 auto;
    max-height: 100%;
    max-width: 100%;
  }

  iframe {
    max-width: 100%;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  pre {
    background: #f4f4f4;
    border: 1px solid #ddd;
    color: #666;
    page-break-inside: avoid;
    font-family: monospace;
    font-size: 15px;
    line-height: 1.6;
    margin-bottom: 1.6em;
    max-width: 100%;
    overflow: auto;
    padding: 10px;
    display: block;
    word-wrap: break-word;
}

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .rotate {
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  width: 1.5em;
}
.rotate div {
     -moz-transform: rotate(-90.0deg);  /* FF3.5+ */
       -o-transform: rotate(-90.0deg);  /* Opera 10.5 */
  -webkit-transform: rotate(-90.0deg);  /* Saf3.1+, Chrome */
             filter:  progid:DXImageTransform.Microsoft.BasicImage(rotation=0.083);  /* IE6,IE7 */
         -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=0.083)"; /* IE8 */
         margin-left: -10em;
         margin-right: -10em;
}

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    max-width: 1100px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
</style>
<!-- ======================================================================= -->

<!-- Start : Google Analytics Code -->
<!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-64069893-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-64069893-4');
</script> -->
<!-- End : Google Analytics Code -->

<script type="text/javascript" src="resources/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
<div max-width=100%>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link rel="icon" type="image/png" href="resources/clvr_icon.png">
  <title>Cross-Domain Transfer via Semantic Skill Imitation</title>
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="canonical" href="https://kpertsch.github.io/" />
  <meta name="referrer" content="no-referrer-when-downgrade" />

  <meta property="og:site_name" content="Cross-Domain Transfer via Semantic Skill Imitation" />
  <meta property="og:type" content="video.other" />
  <meta property="og:title" content="Cross-Domain Transfer via Semantic Skill Imitation" />
  <meta property="og:description" content="Karl Pertsch, Ruta Desai, Vikash Kumar, Franziska Meier, Joseph J. Lim, Dhruv Batra, Akshara Rai. Cross-Domain Transfer via Semantic Skill Imitation. 2022." />
  <meta property="og:url" content="https://kpertsch.github.io/star" />
  <meta property="og:image" content="https://kpertsch.github.io/star/resources/star_teaser.png" />  <!-- UPDATE -->
  <meta property="og:video" content="https://www.youtube.com/v/XXX" />   <!-- UPDATE -->

  <meta property="article:publisher" content="https://kpertsch.github.io/" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Cross-Domain Transfer via Semantic Skill Imitation" />
  <meta name="twitter:description" content="Karl Pertsch, Ruta Desai, Vikash Kumar, Franziska Meier, Joseph J. Lim, Dhruv Batra, Akshara Rai. Cross-Domain Transfer via Semantic Skill Imitation. 2022." />
  <meta name="twitter:url" content="https://kpertsch.github.io/star" />
  <meta name="twitter:image" content="https://kpertsch.github.io/star/resources/star_teaser.png" />   <!-- UPDATE -->
  <meta property="og:image:width" content="1600" />
  <meta property="og:image:height" content="900" />

  <script src="https://www.youtube.com/iframe_api"></script>
  <meta name="twitter:card" content="player" />
  <meta name="twitter:image" content="https://kpertsch.github.io/star/resources/star_teaser.png" />   <!-- UPDATE -->
  <meta name="twitter:player" content="https://www.youtube.com/embed/XXX?rel=0&showinfo=0" />   <!-- UPDATE -->
  <meta name="twitter:player:width" content="640" />
  <meta name="twitter:player:height" content="360" />
</head>

<body>

      <br>
      <center><span style="font-size:44px;font-weight:bold;">Cross-Domain Transfer via Semantic Skill Imitation</span></center><br/>
      <div class="table-like" style="justify-content:space-evenly;max-width:600;margin:0 auto;">
          <div><center><span style="font-size:30px"><a href="https://kpertsch.github.io/" target="_blank">Karl Pertsch<sup>1</sup></a></span></center>
          </div>
          <div><center><span style="font-size:30px"><a href="https://rutadesai.github.io/" target="_blank">Ruta Desai<sup>2</sup></a></span></center>
          </div>
          <div><center><span style="font-size:30px"><a href="https://vikashplus.github.io/" target="_blank">Vikash Kumar<sup>2</sup><br></a></span></center>
          </div>
      </div>
      <div class="table-like" style="justify-content:space-evenly;max-width:900px;margin:-40 auto auto auto;">
          <br/>
          <div><center><span style="font-size:30px"><a href="https://fmeier.github.io/" target="_blank">Franziska Meier<sup>2</sup></a></span></center>
          </div>
          <div><center><span style="font-size:30px"><a href="https://www.clvrai.com/" target="_blank">Joseph J. Lim<sup>3</sup></a></span></center>
          </div>
          <div><center><span style="font-size:30px"><a href="https://faculty.cc.gatech.edu/~dbatra/" target="_blank">Dhruv Batra<sup>2,4</sup></a></span></center>
          </div>
          <div><center><span style="font-size:30px"><a href="https://ai.facebook.com/people/akshara-rai/" target="_blank">Akshara Rai<sup>2</sup></a></span></center>
          </div>
      </div>
      <table align=center width=50% style="padding-top:0px;padding-bottom:0px">
          <tr>
            <td align=center><center><span style="font-size:20px"><sup>1</sup> University of Southern California, <sup>2</sup> Meta AI, <sup>3</sup> KAIST, <sup>4</sup> Georgia Tech</span></center>
          <tr/>
      </table>
      <center><span style="font-size:20px;">Conference on Robot Learning (CoRL), 2022</span></center>

      <div class="table-like" style="justify-content:space-evenly;max-width:500px;margin:auto;padding:5px">
        <div><center><span style="font-size:28px"><a href="https://arxiv.org/abs/2212.07407">[Paper]</a></span></center></div>  <!-- UPDATE -->
        <div><center><span style="font-size:28px"><a href="https://github.com/kpertsch/star">[Code]</a></span></center> </div>   <!-- UPDATE -->
        <!-- <div><center><span style="font-size:28px"><a href='https://youtu.be/w32twGTWvDU'>[Talk (5 min)]</a></span></center> </div> -->
      </div>

      <!-- ### VIDEO ### -->
      <center>
      <iframe width="768" height="432" max-width="100%" src="https://www.youtube.com/embed/ghme5dX-49o?autoplay=1" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></center>
      <!-- width="1024" height="576" -->
      <!-- <iframe width="768" height="432" max-width="100%" src="resources/video.m4v" frameborder="0" allowfullscreen></iframe></center> -->
      <br>

      <!-- <br/> -->
          <!-- <center><img src = "resources/STAR_teaser_gif.gif" width="1000px"></img><br></center> -->
      <!-- <br/> -->

      <div style="width:800px; margin:0 auto;padding:5px" align="justify">
        We propose an approach for <i>semantic</i> imitation, which uses demonstrations from a source domain, e.g. human videos, to accelerate reinforcement learning (RL) in a different target domain, e.g. a robotic manipulator in a simulated kitchen. Instead of imitating low-level actions like joint velocities, our approach imitates the sequence of demonstrated semantic skills like "opening the microwave" or "turning on the stove". This allows us to transfer demonstrations across environments (e.g. real-world to simulated kitchen) and agent embodiments (e.g. bimanual human demonstration to robotic arm). We evaluate on three challenging cross-domain learning problems and match the performance of demonstration-accelerated RL approaches that require in-domain demonstrations. In a simulated kitchen environment, our approach learns long-horizon robot manipulation tasks, using less than 3 minutes of human video demonstrations from a real-world kitchen. This enables scaling robot learning via the reuse of demonstrations, e.g. collected as human videos, for learning in any number of target domains.
      </div>
      <br><hr>


      <!-- ################### OVERVIEW #################### -->
        <center><h1>Overview</h1></center>

        <table align=center width=800px>
        <tr>
        <td style="width:50%">
          <!-- <p style="margin-top:4px;"></p> -->
          <img style="width:600px; float:left" src="resources/star_model.jpeg"/>
        </td>
        <td style="width:3%"></td>
        <td style="width:47%">
          <h2>Learning Semantic Skills</h2>
          <span style="float:right;margin:auto" align="justify">
          We first train a policy for executing semantic skills like "open microwave" or "turn on stove". We assume access to a play-style dataset of diverse agent interactions, e.g. collected via teleoperation or from previous RL runs. Each trajectory has annotations for the executed skills from a set of <i>K</i> semantic skills. We use behavior cloning on randomly sampled subsequences from the dataset to train a skill policy that is conditioned on the semantic skill <i>k</i> and a latent skill variable <i>z</i> that captures execution details of the skill, e.g. <i>how</i> to open the microwave.
          </span>
        </td>
        </tr>
        </table><br>

        <table align=center width=800px>
        <tr>
        <td style="width:47%">
          <h2>Extracting Human Demo Skills</h2>
          <span style="float:left;margin:auto" align="justify">
          Given a human demonstration, our goal is to extract the executed semantic skills in order to imitate them in the target environment. We use a pre-trained action recognition model from a large-scale ego-centric human video datasets (EPIC Kitchens, Damen et al.) to extract step-wise action detections on the human video demonstration. We map the output action detections to our set of <i>K</i> semantic skills and use them to guide learning in the target environment.
          </span>
        </td>
        <td style="width:3%"></td>
        <td style="width:50%">
          <!-- <p style="margin-top:4px;"></p> -->
          <!-- <img style="width:600px; float:right" src="resources/star_model.jpeg"/> -->
          <!-- <iframe width="768" height="432" max-width="100%" src="resources/epic_video.mp4" frameborder="0" allowfullscreen autoplay></iframe></center> -->
          <center><video width="450px" autoplay muted loop> <source src="resources/epic_video.mp4" type="video/mp4"></video></center>
        </td>
        </tr>
        </table><br>

        <table align=center width=800px>
        <tr>
        <td style="width:50%">
          <!-- <p style="margin-top:4px;"></p> -->
          <img style="width:600px; float:left" src="resources/star_policy.jpeg"/>
        </td>
        <td style="width:3%"></td>
        <td style="width:47%">
          <h2>Semantic Skill Transfer</h2>
          <span style="float:right;margin:auto" align="justify">
          Finally, we use the extracted semantic skills from the human video demonstration to guide training in a target environment, e.g. for a robotic kitchen manipulation task. Instead of rigidly following the sequence of demonstrated skills, we use the human demonstrated skills as a prior and regularize the policy towards it during training. This allows the policy to deviate from the demonstrated skill sequence if necessary, e.g. if the initial conditions in the target environment don't exactly match those in the demonstration environment.
          </span>
        </td>
        </tr>
        </table><br>

        <hr>


    <!-- ################### ENVIRONMENTS #################### -->

    <center><h1>Environments</h1></center>
    <!-- <table align=center width=1000px>

        <tr>
        <td style="width:30%">
          <center><h2>Maze Navigation</h2></center>
          <a href="resources/env_videos/maze.mp4"><video src = "resources/env_videos/maze.mp4" width="100%" autoplay muted loop></video></a>
        </td>
        <td style="width:3%"></td>
        <td style="width:30%">
          <center><h2>Kitchen Manipulation</h2></center>
          <a href="resources/env_videos/kitchen.mp4"><video src = "resources/env_videos/kitchen.mp4" width="100%" autoplay muted loop></video></a>
        </td>
        <td style="width:3%"></td>
        <td style="width:30%">
          <center><h2>Office Cleanup</h2></center>
          <a href="resources/env_videos/office.mp4"><video src = "resources/env_videos/office.mp4" width="100%" autoplay muted loop></video></a>
        </td>
        </tr>
        </table><br> -->

      <br/>
          <center><img src = "resources/star_environments.jpeg" width="1000px"></img><br></center>
      <br/>


      <div style="width:800px; margin:0 auto; text-align=right" align="justify">
        We evaluate our approach on three cross-environment imitation problems: a <b>maze navigation task</b> in which the agent needs to follow a sequence of colored rooms in a new maze layout, a <b>simulated kitchen task</b> in which the agent needs to execute a sequence of four kitchen skills in a new kitchen and a <b> human-to-robot kitchen imitation task</b> where the robot needs to imitate a sequence of kitchen task from human demonstration videos. All environments require imitation of the <i>semantic</i> skills from the demonstrations -- regular imitation of low-level actions does not lead to task success.
      </div>
      <hr>


      <!-- ################### EXPERIMENTAL RESULTS #################### -->

      <center><h1>Experimental Results</h1></center>
      <table align=center width=800px>
        <tr>
        <td style="width:60%">
          <!-- <p style="margin-top:4px;"></p> -->
          <img style="width:600px; float:left" src="resources/star_sim_exp.jpeg"/>
        </td>
        <td style="width:3%"></td>
        <td style="width:37%">
          <span style="float:right;margin:auto" align="justify">
          For the two simulated environments, maze navigation and kitchen manipulation, we find that our approach STAR (<b>S</b>emantic <b>T</b>ransfer <b>A</b>ccelerated <b>R</b>L) outperforms skill-based RL without demonstrations (SPiRL) and roughly matches performance to an oracle approach with access to demonstrations in the target domain (SkiLD). 
          </span>
        </td>
        </tr>
      </table>

      <div style="width:800px; margin:0 auto; text-align=right" align="justify">
        This demonstrates that STAR effectively transfers the demonstrated semantic skills to the target environment to accelerate policy learning. Conventional imitation approaches that are not designed for cross-environment imitation (BC) do not make progress on the task since successful task execution in the target environment requires completely different low-level actions compared to the demonstrations.
      </div>
      
      <br/>
      <center><h2>Imitation from Human Demonstration</h2></center>
      <center><img src = "resources/star_human_exp.jpeg" width="1000px"></img><br></center>
      <div style="width:800px; margin:0 auto;" align="justify">
        On the human-to-robot imitation task we show that STAR successfully follows the demonstrated sequence of semantic skills in the target environment. STAR substantially accelerates learning over approaches without demonstation guidance using only three minutes of human video demonstration, collected in an unseen kitchen environment. STAR nearly matches efficiency of an oracle with access to demonstrations in the target environment, demonstrating the promise of cross-domain imitation for scalable robot learning from easy-to-collect human demonstrations.
      </div></br><hr>


      <!-- ################### POLICY ROLLOUTS #################### -->

      <!-- <div style="width:800px; margin:0 auto; text-align=center">
        <center><h1>Qualitative Results</h1></center>
      </div>
      <table align=center width=1000px>
        <tr>
          <td style="width:1%">
            <center><div style="font-size:25px; transform:rotate(270deg)">
            Kitchen Manipulation
            </div></center>
          </td>
          <td style="width:30%">
            <center><h2>SkiLD</h2></center>
            <a href="resources/policy_videos/kitchen_skild.mp4"><video src = "resources/policy_videos/kitchen_skild.mp4" width="100%" autoplay muted loop></video></a>
          </td>
          <td style="width:1%"></td>
          <td style="width:30%">
            <center><h2>SPiRL</h2></center>
            <a href="resources/policy_videos/kitchen_spirl.mp4"><video src = "resources/policy_videos/kitchen_spirl.mp4" width="100%" autoplay muted loop></video></a>
          </td>
          <td style="width:1%"></td>
          <td style="width:30%">
            <center><h2>SkillBC + SAC</h2></center>
            <a href="resources/policy_videos/kitchen_skillBCSAC.mp4"><video src = "resources/policy_videos/kitchen_skillBCSAC.mp4" width="100%" autoplay muted loop></video></a>
          </td>
        </tr>

        <tr>
          <td style="width:1%">
            <center><div style="font-size:25px; transform:rotate(270deg)">
            Office Cleanup
            </div></center>
          </td>
          <td style="width:30%">
            <a href="resources/policy_videos/office_skild.mp4"><video src = "resources/policy_videos/office_skild.mp4" width="100%" autoplay muted loop></video></a>
          </td>
          <td style="width:1%"></td>
          <td style="width:30%">
            <a href="resources/policy_videos/office_spirl.mp4"><video src = "resources/policy_videos/office_spirl.mp4" width="100%" autoplay muted loop></video></a>
          </td>
          <td style="width:1%"></td>
          <td style="width:30%">
            <a href="resources/policy_videos/office_skillBCSAC.mp4"><video src = "resources/policy_videos/office_skillBCSAC.mp4" width="100%" autoplay muted loop></video></a>
          </td>
        </tr>
      </table>
      <br><div style="width:800px; margin:0 auto;" align="justify">
        Rollouts from the trained policies on the robotic manipulation tasks. In the kitchen environment the agent needs to perform four subtasks: open microwave, flip light switch, open slide cabinet, open hinge cabinet. In the office cleanup task it needs to put the correct objects in the correct receptacles. In both environments, our approach SkiLD is the only method that cann solve the full task. SPiRL lacks guidance through the demonstrations and thus solves wrong subtasks and fails at the target task. Skill-based BC with SAC finetuning is brittle and unable to solve more than one subtask. For more qualitative result videos, please check our <a href="https://sites.google.com/view/skill-demo-rl" target="_blank">supplementary website</a>.
      </div></br><hr>
 -->

      <!-- ################### QUANTITATIVE RESULTS #################### -->
      <!-- <div style="width:800px; margin:0 auto; text-align=center">
        <center><h1>Quantitative Results</h1></center>
      </div>
          <center><img src = "resources/skild_quant_results.png" width="1000px"></img><br></center>
      <hr> -->

      <!-- ################### IMITATION RESULTS #################### -->
      <!-- <div style="width:800px; margin:0 auto; text-align=center">
        <center><h1>Imitation Learning Results</h1></center>
      </div>
          <center><img src = "resources/skild_imitation_results.png" width="800px"></img><br></center>
      <br><div style="width:800px; margin:0 auto;" align="justify">
        We apply SkiLD in the pure imitation setting, without access to environment rewards and instead use a GAIL-style reward based on our learned discriminator, which is trained to estimate demonstration support. We show that our approach is able to leverage prior experience through skills for effective imitation of long-horizon tasks. By finetuning the learned discriminator we can further improve performance on the kitchen manipulation task which requires more complex control.
      </div></br><hr> -->

      <!-- ################### CODE #################### -->
      <center id="sourceCode"><h1>Source Code</h1></center>
      <div style="width:800px; margin:0 auto; text-align=right">
      We are planning to soon release our PyTorch on the github page. Stay tuned!
      </div>
      <div class="table-like">
        <span style="font-size:28px"><a href='https://github.com/kpertsch/star'>[GitHub]</a></span>
      </div>
      <br><hr>

      <!-- ################### CITATION #################### -->
      <table align=center width=850px>
        <center><h1>Citation</h1></center>
        <tr>
        <td width=100%>
        <pre><code style="display:block; white-space:pre-wrap">
          @article{pertsch2022star,
            title={Cross-Domain Transfer via Semantic Skill Imitation},
            author={Karl Pertsch and Ruta Desai and Vikash Kumar and Franziska Meier
                    and Joseph J. Lim and Dhruv Batra and Akshara Rai},
            journal={6th Conference on Robot Learning},
            year={2022},
          }
        </code></pre>
          </td>
          </tr>
      </table>
    <br><hr>


      <!-- <div style="width:800px; margin:0 auto; text-align=center">
        <br>
        <center>Code and full paper to be released soon.</center>
      </div> -->
      </table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
</div>
</body>
</html>

